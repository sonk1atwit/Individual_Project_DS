# -*- coding: utf-8 -*-
"""Traffic.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1gi9Y81l9zS6aeJ_xlAek2PFgomCipFpt
"""

# https://www.statista.com/statistics/235786/most-traffic-jam-prone-cities-in-north-america/
# https://www.statista.com/statistics/1305426/us-congestion-cost-driver-urban-area/
# https://www.statista.com/statistics/1305446/us-most-congested-urban-area/

from google.colab import drive
drive.mount('/content/drive')

import matplotlib.pyplot as plt
import pandas as pd

# Read the Excel file
df = pd.read_excel('/content/drive/My Drive/Colab Notebooks/statistic_id235786_most-congested-city-centers-in-the-north-america-2023.xlsx', sheet_name="Data", skiprows=4)

# Clean up the data
cities = df['Unnamed: 1'].dropna()  # Get city names, drop any NaN values
congestion = df['Unnamed: 2'].dropna()  # Get congestion values, drop any NaN values

# Create visualization
plt.figure(figsize=(10, 6))  # Reduced figure size to avoid the dimension error

# Create horizontal bar chart
plt.barh(cities, congestion, color='#2c7fb8')

# Customize the graph
plt.title('Most Congested City Centers in North America 2023', fontsize=12)
plt.xlabel('Congestion Level (%)', fontsize=10)
plt.ylabel('City', fontsize=10)

# Add value labels on the bars
for i, v in enumerate(congestion):
    plt.text(v + 0.5, i, f'{int(v)}%', va='center')

# Adjust layout
plt.margins(x=0.1)
plt.grid(axis='x', linestyle='--', alpha=0.7)

# Adjust figure size to fit content
plt.gcf().set_size_inches(10, 6)
plt.subplots_adjust(left=0.2, right=0.9, top=0.9, bottom=0.1)

# Show the plot
plt.show()

import matplotlib.pyplot as plt
import pandas as pd

# Read the Excel file - corrected file extension from xlsx to xls
df = pd.read_excel('/content/drive/My Drive/Colab Notebooks/statistic_id1305426_cost-per-driver-of-traffic-congestions-in-the-us-by-urban-area-2019.xlsx', sheet_name="Data", skiprows=4)

# Clean up the data
cities = df['Unnamed: 1'].dropna()  # Get city names, drop any NaN values
costs = df['Unnamed: 2'].dropna()  # Get cost values, drop any NaN values

# Create visualization
plt.figure(figsize=(12, 8))

# Create horizontal bar chart
plt.barh(cities, costs, color='#e41a1c')  # Red bars for costs

# Customize the graph
plt.title('Cost per Driver of Traffic Congestion in US Urban Areas (2019)', fontsize=14, pad=20)
plt.xlabel('Cost per Driver ($)', fontsize=12)
plt.ylabel('Urban Area', fontsize=12)

# Add value labels on the bars
for i, v in enumerate(costs):
    plt.text(v + 50, i, f'${int(v):,}', va='center')

# Format the x-axis to show dollar amounts
plt.gca().xaxis.set_major_formatter(plt.FuncFormatter(lambda x, p: f'${int(x):,}'))

# Adjust layout and style
plt.margins(x=0.1)
plt.grid(axis='x', linestyle='--', alpha=0.7)
plt.subplots_adjust(left=0.25, right=0.9, top=0.9, bottom=0.1)

# Show the plot
plt.show()

import matplotlib.pyplot as plt
import pandas as pd

# Read the Excel file
df = pd.read_excel('/content/drive/My Drive/Colab Notebooks/statistic_id1305446_most-congested-urban-area-in-the-us-2019.xlsx', sheet_name="Data", skiprows=4)

# Clean up the data
cities = df['Unnamed: 1'].dropna()  # Get city names, drop any NaN values
congestion = df['Unnamed: 2'].dropna()  # Get congestion values, drop any NaN values

# Create visualization
plt.figure(figsize=(10, 6))  # Reduced figure size to avoid the dimension error

# Create horizontal bar chart
plt.barh(cities, congestion, color='#4daf4a')

# Customize the graph
plt.title('Hours Lost in Congestion in the United States in 2019', fontsize=12)
plt.xlabel('Hours lost in congestion', fontsize=10)
plt.ylabel('Urban Area', fontsize=10)

# Add value labels on the bars
for i, v in enumerate(congestion):
   plt.text(v + 0.5, i, f'{int(v)}', va='center')

# Adjust layout
plt.margins(x=0.1)
plt.grid(axis='x', linestyle='--', alpha=0.7)

# Adjust figure size to fit content
plt.gcf().set_size_inches(10, 6)
plt.subplots_adjust(left=0.2, right=0.9, top=0.9, bottom=0.1)

# Show the plot
plt.show()

from google.colab import drive
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

# Read datasets with proper labels
df_congestion = pd.read_excel('/content/drive/My Drive/Colab Notebooks/statistic_id235786_most-congested-city-centers-in-the-north-america-2023.xlsx', sheet_name="Data", skiprows=4)
df_cost = pd.read_excel('/content/drive/My Drive/Colab Notebooks/statistic_id1305426_cost-per-driver-of-traffic-congestions-in-the-us-by-urban-area-2019.xlsx', sheet_name="Data", skiprows=4)
df_hours = pd.read_excel('/content/drive/My Drive/Colab Notebooks/statistic_id1305446_most-congested-urban-area-in-the-us-2019.xlsx', sheet_name="Data", skiprows=4)

# Rename columns and clean data
def prepare_dataset(df, value_name):
    df_clean = df[['Unnamed: 1', 'Unnamed: 2']].dropna()
    df_clean.columns = ['City', value_name]
    return df_clean

# Prepare each dataset with clear labels
congestion_data = prepare_dataset(df_congestion, 'Congestion_Level_(%)')
cost_data = prepare_dataset(df_cost, 'Cost_Per_Driver_($)')
hours_data = prepare_dataset(df_hours, 'Hours_Lost')

# Function to format the output and create charts
def print_and_plot_dataset(title, data, value_column, color):
    # Print formatted table
    print(f"\033[1m{title}\033[0m")
    max_city_length = data['City'].str.len().max()
    format_string = f"{{:<{max_city_length+5}}}{{:>10}}"

    print(format_string.format('City', value_column))
    print("-" * (max_city_length + 15))

    for _, row in data.iterrows():
        if value_column == 'Cost_Per_Driver_($)':
            print(format_string.format(row['City'], f"${row[value_column]:,.0f}"))
        else:
            print(format_string.format(row['City'], f"{row[value_column]:.0f}"))

    # Create visualization
    plt.figure(figsize=(12, 6))
    bars = plt.barh(data['City'], data[value_column], color=color)

    # Customize the graph
    plt.title(title, fontsize=14, pad=20)
    plt.xlabel(value_column, fontsize=12)

    # Remove spines
    plt.gca().spines['right'].set_visible(False)
    plt.gca().spines['top'].set_visible(False)

    # Add value labels
    for i, bar in enumerate(bars):
        width = bar.get_width()
        if value_column == 'Cost_Per_Driver_($)':
            label = f'${int(width):,}'
        else:
            label = f'{int(width)}'
        plt.text(width + (width * 0.02), bar.get_y() + bar.get_height()/2,
                label, va='center')

    # Adjust layout
    plt.grid(axis='x', linestyle='--', alpha=0.3)
    plt.tight_layout()
    plt.show()
    print("\n" + "="*50 + "\n")

# Display formatted datasets with charts
print_and_plot_dataset("1. City Congestion Levels 2023", congestion_data, 'Congestion_Level_(%)', '#2c7fb8')
print_and_plot_dataset("2. Cost Per Driver 2019", cost_data, 'Cost_Per_Driver_($)', '#e41a1c')
print_and_plot_dataset("3. Hours Lost in Congestion 2019", hours_data, 'Hours_Lost', '#4daf4a')

# Show cities that appear in all three datasets
cities_congestion = set(congestion_data['City'].str.strip())
cities_cost = set(cost_data['City'].str.strip())
cities_hours = set(hours_data['City'].str.strip())

common_cities = cities_congestion.intersection(cities_cost).intersection(cities_hours)

print("\033[1mCities that appear in all three datasets:\033[0m")
print("-" * 45)
for city in sorted(common_cities):
    print(city)

from google.colab import drive
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import r2_score, mean_squared_error
import warnings
warnings.filterwarnings('ignore')

def create_visualizations(merged_df):
    # Set a bigger font size for all plots
    plt.rcParams.update({'font.size': 12})

    # Select only unique metrics for correlation
    metrics = ['Congestion_Level', 'Cost_Per_Driver', 'Hours_Lost', 'Cost_Per_Hour']
    correlation_df = merged_df[metrics].corr()

    # Correlation Heatmap with improved labeling
    plt.figure(figsize=(12, 10))
    sns.heatmap(correlation_df, annot=True, cmap='coolwarm', fmt='.2f',
                square=True, linewidths=0.5, vmin=-1, vmax=1)
    plt.title('Traffic Metrics Correlation Analysis', fontsize=14, pad=20)
    # Rotate x-axis labels for better readability
    plt.xticks(rotation=45, ha='right')
    plt.yticks(rotation=0)
    plt.tight_layout()
    plt.show()

    # Create subplots with better spacing
    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(20, 16))

    # 1. Scatter plot: Congestion Level vs Hours Lost
    sns.scatterplot(data=merged_df, x='Congestion_Level', y='Hours_Lost', ax=ax1, s=100)
    ax1.set_title('How Congestion Affects Time Lost in Traffic', fontsize=14)
    ax1.set_xlabel('Congestion Level (%)', fontsize=12)
    ax1.set_ylabel('Hours Lost per Year', fontsize=12)
    # Add city labels to scatter plot
    for idx, row in merged_df.iterrows():
        ax1.annotate(row['City'],
                    (row['Congestion_Level'], row['Hours_Lost']),
                    xytext=(5, 5), textcoords='offset points')

    # 2. Scatter plot: Cost per Driver vs Hours Lost
    sns.scatterplot(data=merged_df, x='Cost_Per_Driver', y='Hours_Lost', ax=ax2, s=100)
    ax2.set_title('Time Lost vs Cost Impact', fontsize=14)
    ax2.set_xlabel('Annual Cost per Driver ($)', fontsize=12)
    ax2.set_ylabel('Hours Lost per Year', fontsize=12)
    # Add city labels
    for idx, row in merged_df.iterrows():
        ax2.annotate(row['City'],
                    (row['Cost_Per_Driver'], row['Hours_Lost']),
                    xytext=(5, 5), textcoords='offset points')

    # 3. Bar plot: Congestion Level by City
    city_congestion = merged_df.sort_values('Congestion_Level', ascending=True)
    bars3 = ax3.barh(city_congestion['City'], city_congestion['Congestion_Level'],
                     color='#2c7fb8')  # Use a pleasant blue
    ax3.set_title('City Traffic Congestion Comparison', fontsize=14)
    ax3.set_xlabel('Congestion Level (%)', fontsize=12)
    ax3.set_ylabel('City', fontsize=12)
    # Add value labels
    for bar in bars3:
        width = bar.get_width()
        ax3.text(width + 0.5, bar.get_y() + bar.get_height()/2,
                f'{int(width)}%', ha='left', va='center')

    # 4. Bar plot: Cost per Hour by City
    city_cost = merged_df.sort_values('Cost_Per_Driver', ascending=True)
    bars4 = ax4.barh(city_cost['City'], city_cost['Cost_Per_Driver'],
                     color='#e41a1c')  # Use a pleasant red
    ax4.set_title('Annual Cost of Traffic by City', fontsize=14)
    ax4.set_xlabel('Cost per Driver ($)', fontsize=12)
    ax4.set_ylabel('City', fontsize=12)
    # Add value labels
    for bar in bars4:
        width = bar.get_width()
        ax4.text(width + 50, bar.get_y() + bar.get_height()/2,
                f'${int(width):,}', ha='left', va='center')

    # Adjust layout
    plt.tight_layout(pad=3.0)
    plt.show()

    # Print clear summary statistics
    print("\nKey Traffic Statistics by City:")
    print("=" * 50)
    print("\nHighest Congestion Levels:")
    print(merged_df.nlargest(3, 'Congestion_Level')[['City', 'Congestion_Level']].to_string(
        index=False, float_format=lambda x: f'{x:.1f}%'))

    print("\nHighest Annual Costs:")
    print(merged_df.nlargest(3, 'Cost_Per_Driver')[['City', 'Cost_Per_Driver']].to_string(
        index=False, float_format=lambda x: f'${x:,.0f}'))

    print("\nMost Hours Lost in Traffic:")
    print(merged_df.nlargest(3, 'Hours_Lost')[['City', 'Hours_Lost']].to_string(
        index=False, float_format=lambda x: f'{x:.0f} hours'))

# Call the function with your merged dataframe
create_visualizations(merged_df)

from google.colab import drive
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_squared_error
import warnings
warnings.filterwarnings('ignore')

# Mount Google Drive first
drive.mount('/content/drive')

# 1. Research Questions:
print("Research Questions:")
print("1. Can we predict hours lost in traffic based on congestion levels and cost?")
print("2. What is the relationship between congestion levels and economic impact?")
print("3. Which cities show disproportionate costs relative to their congestion levels?\n")

def load_and_clean_data():
    # Read datasets with proper path names
    df_congestion = pd.read_excel('/content/drive/My Drive/Colab Notebooks/statistic_id235786_most-congested-city-centers-in-the-north-america-2023.xlsx', sheet_name="Data", skiprows=4)
    df_cost = pd.read_excel('/content/drive/My Drive/Colab Notebooks/statistic_id1305426_cost-per-driver-of-traffic-congestions-in-the-us-by-urban-area-2019.xlsx', sheet_name="Data", skiprows=4)
    df_hours = pd.read_excel('/content/drive/My Drive/Colab Notebooks/statistic_id1305446_most-congested-urban-area-in-the-us-2019.xlsx', sheet_name="Data", skiprows=4)

    # Clean city names
    def clean_city_name(name):
        if isinstance(name, str):
            return name.split('(')[0].strip()
        return name

    # Prepare datasets
    congestion_data = pd.DataFrame({
        'City': df_congestion['Unnamed: 1'].apply(clean_city_name),
        'Congestion_Level': df_congestion['Unnamed: 2']
    }).dropna()

    cost_data = pd.DataFrame({
        'City': df_cost['Unnamed: 1'].apply(clean_city_name),
        'Cost_Per_Driver': df_cost['Unnamed: 2']
    }).dropna()

    hours_data = pd.DataFrame({
        'City': df_hours['Unnamed: 1'].apply(clean_city_name),
        'Hours_Lost': df_hours['Unnamed: 2']
    }).dropna()

    return congestion_data, cost_data, hours_data

def feature_engineering(merged_df):
    # Create new features
    merged_df['Cost_Per_Hour'] = merged_df['Cost_Per_Driver'] / merged_df['Hours_Lost']
    merged_df['Congestion_Impact'] = merged_df['Congestion_Level'] * merged_df['Hours_Lost'] / 100
    merged_df['Cost_Efficiency'] = merged_df['Cost_Per_Driver'] / merged_df['Congestion_Level']

    print("\nFeature Engineering Results:")
    print("="*50)
    print(merged_df[['City', 'Cost_Per_Hour', 'Congestion_Impact', 'Cost_Efficiency']].head())

    return merged_df

def train_models(merged_df):
    # Prepare features
    X = merged_df[['Congestion_Level', 'Cost_Per_Driver', 'Cost_Per_Hour']]
    y = merged_df['Hours_Lost']

    # Scale features
    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(X)

    # Split data
    X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)

    # Train model
    model = LinearRegression()
    model.fit(X_train, y_train)
    predictions = model.predict(X_test)

    results = {
        'RMSE': np.sqrt(mean_squared_error(y_test, predictions)),
        'Feature_Importance': dict(zip(X.columns, model.coef_))
    }

    return results

def analyze_results(merged_df, model_results):
    # Correlation Analysis
    plt.figure(figsize=(10, 8))
    correlation_matrix = merged_df[['Congestion_Level', 'Cost_Per_Driver', 'Hours_Lost']].corr()
    sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt='.2f')
    plt.title('Correlation Between Traffic Metrics')
    plt.tight_layout()
    plt.show()

    # Print model performance
    print("\nModel Performance:")
    print("="*50)
    if 'RMSE' in model_results and model_results['RMSE'] is not None:
        print(f"RMSE: {model_results['RMSE']:.4f}")
    else:
        print("RMSE: Unable to calculate")

    print("\nFeature Importance:")
    for feature, importance in model_results['Feature_Importance'].items():
        print(f"{feature}: {importance:.4f}")

# Main execution
if __name__ == "__main__":
    # Load and clean data
    congestion_data, cost_data, hours_data = load_and_clean_data()

    # Merge datasets
    merged_df = pd.merge(congestion_data, cost_data, on='City', how='inner')
    merged_df = pd.merge(merged_df, hours_data, on='City', how='inner')

    # Feature engineering
    merged_df = feature_engineering(merged_df)

    # Train model and get results
    model_results = train_models(merged_df)

    # Analyze results
    analyze_results(merged_df, model_results)

    # Print key findings
    print("\nKey Findings:")
    print("="*50)
    print("\nMost Cost-Efficient Cities:")
    print(merged_df.nsmallest(3, 'Cost_Efficiency')[['City', 'Cost_Efficiency']])

    print("\nHighest Impact Cities:")
    print(merged_df.nlargest(3, 'Congestion_Impact')[['City', 'Congestion_Impact']])